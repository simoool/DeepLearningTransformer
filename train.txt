DEF initialize_weights

Controlla che il transformer abbia l'attributo "weight" e che la sua dimensione sia > 1.
Se è vero, inizializza i pesi con una distibuzione uniforme di tipo xavier uniform.
    --> xavier uniform: distribuzione uniforme tra -x e x, dove x=sqrt(6/(fan_in+fan_out)), e fan_in e fan_out sono rispettivamente il numero di unità di input e di output nel tensore di pesi.






DEF save_dictionary

Apre come file (f) i dizionari in entrambe le lingue. Si tratta dei file che contengono tutte le frasi del dataset, fino a MAX_FILE_SIZE.
Quindi scrive la rappresentazione pickle dei dizionari sui rispettivi file f.



DEF __init__


1) self.input_lang_dic, self.output_lang_dic, self.input_lang_list, self.output_lang_list = load_files(lang1, lang2, data_directory, reverse, self.MAX_FILE_SIZE, self.MAX_LENGTH)

Carica i file chiamando la funzione load_files, che riceve:
- lingua input (italian);
- lingua output (english);
- directory dataset (data);
- reverse;
- MAX_FILE_SIZE;
- MAX_LENGTH.

    --> load_files:
        - apre entrambi i dataset e salva sulle liste lang1_list e lang2_list un numero di frasi pari a MAX_FILE_SIZE.
        - normalizza le liste appena create e salva i risultati su lang1_normalized e lang2_normalized.
        - per ogni frase normalizzata crea una lista (tokens1/2), dove ogni elemento corrisponde a una parola di quella frase (es: ['ripresa', 'della', 'sessione']). Dopodichè controlla che le frasi correnti (in entrambe le lingue) siano di lunghezza <= MAX_LENGTH (60), e nel caso le agginge alle liste lang1_sentences e lang2_sentences.
        - elimina lang1_normalized e lang2_normalized, che dopo il passo precedente non servono più.
        - controlla il boolean reverse. Se True scambia lang1 e lang2, altrimenti no.
        Restituisce, in ordine:
        - input_dic: oggetto Dictionary relativo a lang1 o lang2, in base a reverse;
        - output_dic: oggetto Dictionary relativo a lang1 o lang2, in base a reverse;
        - lang1_sentences: lista di frasi di input o output (in base a reverse) normalizzate di lunghezza <= MAX_LENGTH;
        - lang2_sentences: lista di frasi di input o output (in base a reverse) normalizzate di lunghezza <= MAX_LENGTH;




2) for sentence in self.input_lang_list:
        self.input_lang_dic.add_sentence(sentence)
    for sentence in self.output_lang_list:
        self.output_lang_dic.add_sentence(sentence)

Per ogni frase in input_lang_list (corrispondente a lang1_sentences), aggiunge quella frase a input_lang_dic (corrispondente a input_dic).
Per ogni frase in output_lang_list (corrispondente a lang2_sentences), aggiunge quella frase a output_lang_dic (corrispondente a output_dic).
--> input_lang_dic conterrà quindi le prime MAX_FILE_SIZE frasi del dataset inglese, output_lang_dic quelle del dataset italiano (con reverse=1).




3) self.save_dictionary(self.input_lang_dic, input=True)
   self.save_dictionary(self.output_lang_dic, input=False)

Chiama il metodo save_dictionary:
--> apre saved_models-->english2italian-->input_dic.pkl come file, e scrive su di esso la rappresentazione pickle del dizionario input_lang_dic.
--> apre saved_models-->italian2english-->output_dic.pkl come file, e scrive su di esso la rappresentazione pickle del dizionario output_lang_dic.




4) self.tokenized_input_lang = [tokenize(sentence, self.input_lang_dic, self.MAX_LENGTH) for sentence in self.input_lang_list]
   self.tokenized_output_lang = [tokenize(sentence, self.output_lang_dic, self.MAX_LENGTH) for sentence in self.output_lang_list]

Per ogni frase in input_lang_list (lista di frasi normalizzate), chiama il metodo tokenize, che riceve:
- sentence (frase corrente);
- self.input_lang_dic (dizionario con le frasi in inglese tokenizzate) OPPURE self.output_lang_dic (dizionario con le frasi in italiano tokenizzate);
- self.MAX_LENGTH (60);

    --> tokenize:
        - Separa le parole della frase corrente salvandole in una lista split_sentence.
        - Partendo da split_sentence crea una lista di token corrispondenti agli indici delle parole, aggiungendo i token SOS, EOS e PAD.
        Restituisce:
        - token

Il risultato viene salvato in tokenized_input_lang, che sarà quindi una lista di liste in cui ogni lista è composta da 62 elementi (SOS, EOS, indici+PAD).
Lo stesso viene fatto anche per l'output.




5) self.batch_size = batch_size

Inizializza batch_size (128), che è un iperparametro.




6) self.data_loader = load_batches(self.tokenized_input_lang, self.tokenized_output_lang, self.batch_size, self.device)

Chiama il metodo load_batches, che riceve:
- tokenized_input_lang;
- tokenized_output_lang;
- batch_size;

    --> load_batches:
        - inizializza data_loader come lista vuota.
        - itera con un for che va da 0 a len(input_lang) spostandosi ad ogni iterazione di batch_size.
        - salva in seq_length il passo di spostamento, che sarà sempre uguale a batch_size (?).
        - salva in input_batch tutti gli elementi di input_lang (tokenized_input_lang) che vanno da i a i+seq_length. Lo stesso viene fatto anche per l'output salvando in target_batch.
        - trasforma input_batch e target_batch nei tensori input_tensor e target_tensor rispettivamente.
        - ad ogni iterazione aggiunge a data_loader le coppie [input_tensor, target_tensor]
        Restituisce:
        - data_loader

Salva quindi il risultato in self.data_loader.




7) input_size = self.input_lang_dic.n_count
   output_size = self.output_lang_dic.n_count

Salva in input_size e output_size il numero di parole presenti nei due dizionari.




8) encoder_part = Encoder(input_size, hidden_size, encoder_layers, encoder_heads, encoder_ff_size, encoder_dropout, self.device)
   decoder_part = Decoder(output_size, hidden_size, decoder_layers, decoder_heads, decoder_ff_size, decoder_dropout, self.device)
   self.transformer = Transformer(encoder_part, decoder_part, self.device, PAD_TOKEN).to(self.device)
   self.transformer.apply(self.initialize_weights)

Definisce gli oggetti encoder_part, decoder_part e transformer, quindi applica la funzione initialize_weights a quest'ultimo.




9) self.loss_func = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)

Crea l'oggetto loss_func che servirà per il calcolo della Cross Entropy, specificando che i valori corrispondenti a PAD_TOKEN devono essere ignorati in quanto non contribuiscono al calcolo del gradiente.




10) self.optimizer = optim.Adam(self.transformer.parameters(), lr=lr)

Crea l'oggetto optimizer che servirà per l'ottimizzazione Adam, passando i parametri del transformer e il learning rate (iperparametro).






DEF TRAIN

1) Definisce start_time come tempo di inizio.


2) Per ogni epoca
    - Mescola il data_loader per prevenire l'overfitting.
    - Inizializza nuovamente start.time e imposta train_loss=0.
    - Per ogni tensore di input e di target di data_loader:
        - imposta il gradiente di optimizer a zero;
        - passa attraverso il transformer calcolando l'output e la sua dimensione;
        - esegue una flatten sull'output e omette il token SOS dal target;
        - calcola la loss;
        - effettua la backpropagation calcolando la norma del gradiente e facendo un singolo step di ottimizzazione;
        - aggiorna la loss;
    - Aggiorna la loss dividendola per la lunghezza del data_loader (perchè?)
    - Calcola l'end_time;
    - Salva il transformer;
    - Stampa il risultato.